{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UxwCayAE1oae",
        "outputId": "158787e5-f189-488a-b2cc-eb4b3bacaa50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.11)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.2.post1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.2)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.4.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.66.6)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.11.2)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.3)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.8.0)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.46.2)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Collecting numpy==1.22.0 (from TTS)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.60.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.16.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (4.4.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS)\n",
            "  Downloading librosa-0.10.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=1.11.2 (from TTS)\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (3.16.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (2.17.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.26.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.20.3)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->TTS)\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.0.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=71a06cf98cb5ac4abe9f9ab2fd5925b24b4a4576ef28efbb4859dd8a3b3ada2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=149698bc6ae816504e4eff4f6e6f28592fc6af6794dd1930c5b2813809334a0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=60783b36941282a06b0fe21b07ecd2112e936c09536081d310f6b1316eb3b7f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=29f83aa2d825acbcdd536d9ecfdbed9361bd893f4669c585734e6b813b680cbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=6e0d5e1036ff51efe9779d6b7fa63c40868ccaa5499b7c3071f522bd4200d0c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=4ccd7b5f96e704bc4295ebccc62556fe8345dd533b99fce0ea5e9756454b71f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/80/5f/775b357ae61d7cb68793327c7470d848715cbc60bb373af8dd\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=12bd048bc781b263650da137e444ba2956ef2fdc7260a09de00b3f7ecc02f030\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8d/b7/d484d224facd899ed188e00374f25dd3f19d1a3f53da6517bd\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=ebb5509b40cc265b0d78ba298d1ad80cd8ebbd945c9b2662f86863f9866c1763\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/bd/96/5ddde14e8e6932a96f12c5ab5de62b619d39e2507d7daf5188\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=ff67aa46925a7ffd018c07fe79bbcd47873c03bda021e2862d9b51806107d4d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, numpy, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, scipy, pandas, g2pkk, dateparser, contourpy, trainer, gruut, pynndescent, librosa, encodec, umap-learn, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.1\n",
            "    Uninstalling contourpy-1.3.1:\n",
            "      Successfully uninstalled contourpy-1.3.1\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "astropy 6.1.6 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.22.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.22.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 contourpy-1.2.1 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 scipy-1.11.4 sudachidict-core-20241021 sudachipy-0.6.9 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "c5d95e44e6af46a7a7af4cc1fc123638",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62CK6xtT2Qrx",
        "outputId": "0821db3f-79d6-4c4e-d813-4e67e9c8ae5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.0\n",
            "    Uninstalling numpy-1.22.0:\n",
            "      Successfully uninstalled numpy-1.22.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 2.0.2 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "gruut 2.2.3 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pytensor 2.26.3 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.2 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2 tensorboard-2.18.0 tensorflow-2.18.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.46.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "i5hmW_KP2W-Y",
        "outputId": "b0bef2a9-6fe5-4fd8-9c28-2964395a0ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-2.0.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.22.0\n",
            "  Using cached numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Using cached numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "astropy 6.1.6 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.22.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.22.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.22.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a20273059be04e30a8227afa5b83d0cb",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.22.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebuf_cai2PtA",
        "outputId": "4882639f-7efb-4cb8-ac46-6fb8b55d5da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: PyPDF2, flask-ngrok, flask-cors\n",
            "Successfully installed PyPDF2-3.0.1 flask-cors-5.0.0 flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok flask openai PyPDF2 soundfile flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8KvDkNvWeZv",
        "outputId": "54d271b5-97d8-461c-9970-7f01bbe832d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyngrok\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWn_lLZWFv4Y",
        "outputId": "46ab678e-6c0c-4490-c4e6-2c23529fc3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "ngrok.set_auth_token('2p5AUtW19kWwkyDVLu4ld6vq7jb_6AdiiKpgobFqsZbNeB1CD')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSqMT2uhw-JO"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_file, Response\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import random\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from TTS.api import TTS\n",
        "import openai\n",
        "import torch\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}}, allow_headers=[\"Content-Type\", \"Authorization\", \"ngrok-skip-browser-warning\"])\n",
        "run_with_ngrok(app)  # Run with ngrok for public access\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"Flask server is running with ngrok!\"\n",
        "\n",
        "# Set OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAITjS4UFHw2",
        "outputId": "888c3006-eeab-4047-fe0d-1c026f0a5f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
            " | | > y\n",
            " > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1.87G/1.87G [00:44<00:00, 42.4MiB/s]\n",
            "100%|██████████| 1.87G/1.87G [00:44<00:00, 41.9MiB/s]\n",
            "100%|██████████| 4.37k/4.37k [00:00<00:00, 26.5kiB/s]\n",
            " 57%|█████▋    | 206k/361k [00:00<00:00, 1.33MiB/s]\n",
            "100%|██████████| 361k/361k [00:00<00:00, 950kiB/s] \n",
            "100%|██████████| 32.0/32.0 [00:00<00:00, 90.7iB/s]\n",
            " 43%|████▎     | 3.33M/7.75M [00:00<00:00, 33.3MiB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Model's license - CPML\n",
            " > Check https://coqui.ai/cpml.txt for more info.\n",
            " > Using model: xtts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 7.75M/7.75M [00:14<00:00, 33.3MiB/s]/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.speakers = torch.load(speaker_file_path)\n",
            "/usr/local/lib/python3.10/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n",
            "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TTS(\n",
              "  (synthesizer): Synthesizer(\n",
              "    (tts_model): Xtts(\n",
              "      (gpt): GPT(\n",
              "        (conditioning_encoder): ConditioningEncoder(\n",
              "          (init): Conv1d(80, 1024, kernel_size=(1,), stride=(1,))\n",
              "          (attn): Sequential(\n",
              "            (0): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (1): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (2): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (4): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (5): AttentionBlock(\n",
              "              (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
              "              (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
              "              (attention): QKVAttention()\n",
              "              (x_proj): Identity()\n",
              "              (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (conditioning_dropout): Dropout1d(p=0.1, inplace=False)\n",
              "        (text_embedding): Embedding(6681, 1024)\n",
              "        (mel_embedding): Embedding(1026, 1024)\n",
              "        (gpt): GPT2Model(\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "          (h): ModuleList(\n",
              "            (0-29): 30 x GPT2Block(\n",
              "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): GPT2SdpaAttention(\n",
              "                (c_attn): Conv1D(nf=3072, nx=1024)\n",
              "                (c_proj): Conv1D(nf=1024, nx=1024)\n",
              "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): GPT2MLP(\n",
              "                (c_fc): Conv1D(nf=4096, nx=1024)\n",
              "                (c_proj): Conv1D(nf=1024, nx=4096)\n",
              "                (act): NewGELUActivation()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (wte): Embedding(1026, 1024)\n",
              "        )\n",
              "        (mel_pos_embedding): LearnedPositionEmbeddings(\n",
              "          (emb): Embedding(608, 1024)\n",
              "        )\n",
              "        (text_pos_embedding): LearnedPositionEmbeddings(\n",
              "          (emb): Embedding(404, 1024)\n",
              "        )\n",
              "        (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (text_head): Linear(in_features=1024, out_features=6681, bias=True)\n",
              "        (mel_head): Linear(in_features=1024, out_features=1026, bias=True)\n",
              "        (conditioning_perceiver): PerceiverResampler(\n",
              "          (proj_context): Identity()\n",
              "          (layers): ModuleList(\n",
              "            (0-1): 2 x ModuleList(\n",
              "              (0): Attention(\n",
              "                (attend): Attend(\n",
              "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "                (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              )\n",
              "              (1): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=5460, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2730, out_features=1024, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (norm): RMSNorm()\n",
              "        )\n",
              "        (gpt_inference): GPT2InferenceModel(\n",
              "          (transformer): GPT2Model(\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "            (h): ModuleList(\n",
              "              (0-29): 30 x GPT2Block(\n",
              "                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                (attn): GPT2SdpaAttention(\n",
              "                  (c_attn): Conv1D(nf=3072, nx=1024)\n",
              "                  (c_proj): Conv1D(nf=1024, nx=1024)\n",
              "                  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                (mlp): GPT2MLP(\n",
              "                  (c_fc): Conv1D(nf=4096, nx=1024)\n",
              "                  (c_proj): Conv1D(nf=1024, nx=4096)\n",
              "                  (act): NewGELUActivation()\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (wte): Embedding(1026, 1024)\n",
              "          )\n",
              "          (pos_embedding): LearnedPositionEmbeddings(\n",
              "            (emb): Embedding(608, 1024)\n",
              "          )\n",
              "          (embeddings): Embedding(1026, 1024)\n",
              "          (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (lm_head): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): Linear(in_features=1024, out_features=1026, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (hifigan_decoder): HifiDecoder(\n",
              "        (waveform_decoder): HifiganGenerator(\n",
              "          (conv_pre): Conv1d(1024, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
              "          (ups): ModuleList(\n",
              "            (0): ParametrizedConvTranspose1d(\n",
              "              512, 256, kernel_size=(16,), stride=(8,), padding=(4,)\n",
              "              (parametrizations): ModuleDict(\n",
              "                (weight): ParametrizationList(\n",
              "                  (0): _WeightNorm()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): ParametrizedConvTranspose1d(\n",
              "              256, 128, kernel_size=(16,), stride=(8,), padding=(4,)\n",
              "              (parametrizations): ModuleDict(\n",
              "                (weight): ParametrizationList(\n",
              "                  (0): _WeightNorm()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): ParametrizedConvTranspose1d(\n",
              "              128, 64, kernel_size=(4,), stride=(2,), padding=(1,)\n",
              "              (parametrizations): ModuleDict(\n",
              "                (weight): ParametrizationList(\n",
              "                  (0): _WeightNorm()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): ParametrizedConvTranspose1d(\n",
              "              64, 32, kernel_size=(4,), stride=(2,), padding=(1,)\n",
              "              (parametrizations): ModuleDict(\n",
              "                (weight): ParametrizationList(\n",
              "                  (0): _WeightNorm()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (resblocks): ModuleList(\n",
              "            (0): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (4): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (5): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (6): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (7): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (8): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (9): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (10): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (11): ResBlock1(\n",
              "              (convs1): ModuleList(\n",
              "                (0): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (1): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "                (2): ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (convs2): ModuleList(\n",
              "                (0-2): 3 x ParametrizedConv1d(\n",
              "                  32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
              "                  (parametrizations): ModuleDict(\n",
              "                    (weight): ParametrizationList(\n",
              "                      (0): _WeightNorm()\n",
              "                    )\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
              "          (cond_layer): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "          (conds): ModuleList(\n",
              "            (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
              "            (1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "            (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))\n",
              "            (3): Conv1d(512, 32, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (speaker_encoder): ResNetSpeakerEncoder(\n",
              "          (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (layer1): Sequential(\n",
              "            (0): SEBasicBlock(\n",
              "              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=4, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=4, out_features=32, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): SEBasicBlock(\n",
              "              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=4, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=4, out_features=32, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): SEBasicBlock(\n",
              "              (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=4, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=4, out_features=32, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (layer2): Sequential(\n",
              "            (0): SEBasicBlock(\n",
              "              (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=64, out_features=8, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=8, out_features=64, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "              (downsample): Sequential(\n",
              "                (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): SEBasicBlock(\n",
              "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=64, out_features=8, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=8, out_features=64, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): SEBasicBlock(\n",
              "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=64, out_features=8, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=8, out_features=64, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): SEBasicBlock(\n",
              "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=64, out_features=8, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=8, out_features=64, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (layer3): Sequential(\n",
              "            (0): SEBasicBlock(\n",
              "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "              (downsample): Sequential(\n",
              "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (4): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (5): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=128, out_features=16, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=16, out_features=128, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (layer4): Sequential(\n",
              "            (0): SEBasicBlock(\n",
              "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=256, out_features=32, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=32, out_features=256, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "              (downsample): Sequential(\n",
              "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (1): SEBasicBlock(\n",
              "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=256, out_features=32, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=32, out_features=256, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): SEBasicBlock(\n",
              "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (relu): ReLU(inplace=True)\n",
              "              (se): SELayer(\n",
              "                (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "                (fc): Sequential(\n",
              "                  (0): Linear(in_features=256, out_features=32, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=32, out_features=256, bias=True)\n",
              "                  (3): Sigmoid()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (instancenorm): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "          (torch_spec): Sequential(\n",
              "            (0): PreEmphasis()\n",
              "            (1): MelSpectrogram(\n",
              "              (spectrogram): Spectrogram()\n",
              "              (mel_scale): MelScale()\n",
              "            )\n",
              "          )\n",
              "          (attention): Sequential(\n",
              "            (0): Conv1d(2048, 128, kernel_size=(1,), stride=(1,))\n",
              "            (1): ReLU()\n",
              "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (3): Conv1d(128, 2048, kernel_size=(1,), stride=(1,))\n",
              "            (4): Softmax(dim=2)\n",
              "          )\n",
              "          (fc): Linear(in_features=4096, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize TTS model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
        "tts.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWI8DL1lzred"
      },
      "outputs": [],
      "source": [
        "# Helper function to save data to JSON\n",
        "def save_to_json(extracted_data, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(extracted_data, file, indent=4, ensure_ascii=False)\n",
        "    print(f\"Dataset saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpdjxuF5LjTG"
      },
      "outputs": [],
      "source": [
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_and_process():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Step 1: Extract text from the PDF using PyPDF2\n",
        "    reader = PdfReader(file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    # Step 2: Chunk text for GPT processing\n",
        "    def chunk_text(text, max_tokens=3000):\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for word in words:\n",
        "            current_length += len(word) + 1  # +1 for space\n",
        "            if current_length > max_tokens:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = len(word) + 1\n",
        "            current_chunk.append(word)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    text_chunks = chunk_text(text)\n",
        "\n",
        "    # Step 3: Format each chunk using GPT\n",
        "    def format_screenplay(chunk):\n",
        "        prompt = f\"\"\"\n",
        "        You are a screenplay formatter. Format the following text into a screenplay style:\n",
        "        - Enclose all narration and stage directions in square brackets [ ].\n",
        "        - Retain speaker names in uppercase followed by a colon (e.g., JACOB:).\n",
        "        - Do not change the content of the dialogue but convert it to lowercase.\n",
        "        - Ensure the dialogue remains faithful to the original content.\n",
        "        - After every dialogue, add a new line for separation.\n",
        "        - Exclude extra information such as file metadata, scene numbers, act numbers, or character descriptions.\n",
        "\n",
        "        Text to format:\n",
        "        {chunk}\n",
        "        \"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"chatgpt-4o-latest\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that formats text into screenplay style.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    formatted_chunks = [format_screenplay(chunk) for chunk in text_chunks]\n",
        "    final_formatted_text = \"\\n\\n\".join(formatted_chunks)\n",
        "\n",
        "    # Save formatted text to a file\n",
        "    formatted_text_filename = \"formatted_screenplay.txt\"\n",
        "    with open(formatted_text_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
        "        output_file.write(final_formatted_text)\n",
        "\n",
        "    # Step 4: Extract speakers and quotes from the formatted text\n",
        "    def split_quote_into_chunks(quote, max_length=150):\n",
        "        quote = re.sub(r'[\\(\\)\\[\\]]', ' ', quote)\n",
        "        # Split the quote into sentences using regex\n",
        "        sentence_endings = re.compile(r'([.!?][\"\\']?\\s)')\n",
        "        # Add a space at the end to match the last sentence\n",
        "        sentences = sentence_endings.split(quote + ' ')\n",
        "        # Reconstruct the sentences properly\n",
        "        full_sentences = []\n",
        "        for i in range(0, len(sentences) - 1, 2):\n",
        "            sentence = sentences[i] + sentences[i+1]\n",
        "            full_sentences.append(sentence.strip())\n",
        "        # Handle any remaining text\n",
        "        if len(sentences) % 2 != 0:\n",
        "            full_sentences.append(sentences[-1].strip())\n",
        "        # Group sentences into chunks\n",
        "        chunks = {}\n",
        "        current_chunk = ''\n",
        "        chunk_index = 0\n",
        "        for sentence in full_sentences:\n",
        "            if len(current_chunk) + len(sentence) + 1 <= max_length:\n",
        "                if current_chunk:\n",
        "                    current_chunk += ' ' + sentence\n",
        "                else:\n",
        "                    current_chunk = sentence\n",
        "            else:\n",
        "                # Save the current chunk\n",
        "                chunks[str(chunk_index)] = current_chunk.strip()\n",
        "                chunk_index += 1\n",
        "                current_chunk = sentence\n",
        "        if current_chunk:\n",
        "            chunks[str(chunk_index)] = current_chunk.strip()\n",
        "        return chunks\n",
        "\n",
        "    def extract_speaker_and_quote(text):\n",
        "\n",
        "        output = []\n",
        "        # Remove leading and trailing quotes if present\n",
        "        text = text.strip('\"').strip()\n",
        "        # Replace newline characters with spaces\n",
        "        text = text.replace('\\n', ' ')\n",
        "        # Define the pattern to match narration and dialogue blocks\n",
        "        pattern = r'(\\(.*?\\)|\\[.*?\\])|([A-Z ]+(?:\\:|\\.)\\s*(?:\\(.*?\\)|\\[.*?\\])?.*?(?=(?:[A-Z ]+(?:\\:|\\.)|\\(.*?\\)|\\[.*?\\]|$)))'\n",
        "        # Find all matches in the text\n",
        "        matches = re.finditer(pattern, text, re.DOTALL)\n",
        "        for match in matches:\n",
        "            narration = match.group(1)\n",
        "            dialogue = match.group(2)\n",
        "            if narration:\n",
        "                # Assign narration to narrator\n",
        "                narration = narration.strip()\n",
        "                narration = re.sub(r'[\\(\\)\\[\\]]', ' ', narration)\n",
        "                # Check if narration needs splitting\n",
        "                if len(narration) > 150:\n",
        "                    chunks = split_quote_into_chunks(narration)\n",
        "                    output.append({\n",
        "                        'speaker': 'narrator',\n",
        "                        'quote': chunks\n",
        "                    })\n",
        "                else:\n",
        "                    output.append({\n",
        "                        'speaker': 'narrator',\n",
        "                        'quote': {\n",
        "                            '0': narration\n",
        "                        }\n",
        "                    })\n",
        "            elif dialogue:\n",
        "                # Process dialogue line\n",
        "                dialogue = dialogue.strip()\n",
        "                # Match the speaker and the rest of the dialogue\n",
        "                speaker_match = re.match(r'^([A-Z ]+)(\\:|\\.)(.*)', dialogue, re.DOTALL)\n",
        "                if speaker_match:\n",
        "                    speaker = speaker_match.group(1).strip()\n",
        "                    rest = speaker_match.group(3).strip()\n",
        "                    # Check if rest starts with brackets\n",
        "                    bracket_match = re.match(r'^(\\(.*?\\)|\\[.*?\\])\\s*(.*)', rest, re.DOTALL)\n",
        "                    if bracket_match:\n",
        "                        # Assign initial bracketed text to narrator\n",
        "                        narration_text = f\"{speaker}{bracket_match.group(1)}\"\n",
        "                        narration_text = re.sub(r'[\\(\\)\\[\\]]', ' ', narration_text)\n",
        "                        # Check if narration needs splitting\n",
        "                        if len(narration_text) > 150:\n",
        "                            chunks = split_quote_into_chunks(narration_text)\n",
        "                            output.append({\n",
        "                                'speaker': 'narrator',\n",
        "                                'quote': chunks\n",
        "                            })\n",
        "                        else:\n",
        "                            output.append({\n",
        "                                'speaker': 'narrator',\n",
        "                                'quote': {\n",
        "                                    '0': narration_text\n",
        "                                }\n",
        "                            })\n",
        "                        rest = bracket_match.group(2).strip()\n",
        "                    else:\n",
        "                        narration_text = ''\n",
        "                    # Split the rest of the dialogue if brackets are found in the middle or end\n",
        "                    parts = re.split(r'(\\(.*?\\)|\\[.*?\\])', rest)\n",
        "                    speech = ''\n",
        "                    for i, part in enumerate(parts):\n",
        "                        if i % 2 == 0:\n",
        "                            speech += part.strip() + ' '\n",
        "                        else:\n",
        "                            # Bracketed text found, assign to narrator\n",
        "                            narration_text = part.strip()\n",
        "                            narration_text = re.sub(r'[\\(\\)\\[\\]]', ' ', narration_text)\n",
        "                            if speech.strip():\n",
        "                                # Check if speech needs splitting\n",
        "                                speech_clean = re.sub(r'[\\(\\)\\[\\]]', ' ', speech.strip())\n",
        "                                if len(speech.strip()) > 150:\n",
        "                                    chunks = split_quote_into_chunks(speech.strip())\n",
        "                                    output.append({\n",
        "                                        'speaker': speaker,\n",
        "                                        'quote': chunks\n",
        "                                    })\n",
        "                                else:\n",
        "                                    output.append({\n",
        "                                        'speaker': speaker,\n",
        "                                        'quote': {\n",
        "                                            '0': speech.strip()\n",
        "                                        }\n",
        "                                    })\n",
        "                            # Check if narration needs splitting\n",
        "                            if len(narration_text) > 150:\n",
        "                                chunks = split_quote_into_chunks(narration_text)\n",
        "                                output.append({\n",
        "                                    'speaker': 'narrator',\n",
        "                                    'quote': chunks\n",
        "                                })\n",
        "                            else:\n",
        "                                output.append({\n",
        "                                    'speaker': 'narrator',\n",
        "                                    'quote': {\n",
        "                                        '0': narration_text\n",
        "                                    }\n",
        "                                })\n",
        "                            speech = ''\n",
        "                    if speech.strip():\n",
        "                        # Check if speech needs splitting\n",
        "                        speech_clean = re.sub(r'[\\(\\)\\[\\]]', ' ', speech.strip())\n",
        "                        if len(speech.strip()) > 150:\n",
        "                            chunks = split_quote_into_chunks(speech.strip())\n",
        "                            output.append({\n",
        "                                'speaker': speaker,\n",
        "                                'quote': chunks\n",
        "                            })\n",
        "                        else:\n",
        "                            output.append({\n",
        "                                'speaker': speaker,\n",
        "                                'quote': {\n",
        "                                    '0': speech.strip()\n",
        "                                }\n",
        "                            })\n",
        "                else:\n",
        "                    # Assign any unmatched dialogue to narrator\n",
        "                    dialogue = dialogue.strip()\n",
        "                    dialogue = re.sub(r'[\\(\\)\\[\\]]', ' ', dialogue)\n",
        "                    # Check if dialogue needs splitting\n",
        "                    if len(dialogue) > 150:\n",
        "                        chunks = split_quote_into_chunks(dialogue)\n",
        "                        output.append({\n",
        "                            'speaker': 'narrator',\n",
        "                            'quote': chunks\n",
        "                        })\n",
        "                    else:\n",
        "                        output.append({\n",
        "                            'speaker': 'narrator',\n",
        "                            'quote': {\n",
        "                                '0': dialogue\n",
        "                            }\n",
        "                        })\n",
        "        return output\n",
        "\n",
        "    extracted_data = extract_speaker_and_quote(final_formatted_text)\n",
        "    save_to_json(extracted_data, \"play_speaker_quote_pair.json\")\n",
        "\n",
        "    # Step 5: Allocate gender to speakers using Genderize API\n",
        "    with open('play_speaker_quote_pair.json', 'r', encoding='utf-8') as file:\n",
        "        extracted_data = json.load(file)\n",
        "\n",
        "    speaker_data = {}\n",
        "\n",
        "    for entry in extracted_data:\n",
        "        speaker = entry[\"speaker\"]\n",
        "        quote_count = len(entry[\"quote\"])\n",
        "\n",
        "        if speaker not in speaker_data:\n",
        "            speaker_data[speaker] = {\"dialogue_count\": 0, \"gender\": \"unknown\"}\n",
        "\n",
        "        speaker_data[speaker][\"dialogue_count\"] += quote_count\n",
        "\n",
        "    for speaker in speaker_data:\n",
        "        speaker_name = speaker.split()[0]\n",
        "        response = requests.get(\"https://api.genderize.io\", params={\"name\": speaker_name})\n",
        "        data = response.json()\n",
        "        gender = data.get(\"gender\", \"unknown\")\n",
        "        if gender == \"unknown\":\n",
        "            gender = random.choice([\"male\", \"female\"])\n",
        "        speaker_data[speaker][\"gender\"] = gender\n",
        "\n",
        "    speaker_array_sorted = sorted(\n",
        "        [{\"speaker\": speaker, \"dialogue_count\": data[\"dialogue_count\"], \"gender\": data[\"gender\"]}\n",
        "         for speaker, data in speaker_data.items()],\n",
        "        key=lambda x: x[\"dialogue_count\"], reverse=True\n",
        "    )\n",
        "\n",
        "    save_to_json(speaker_array_sorted, 'speaker_data.json')\n",
        "\n",
        "    # Step 6: Generate audio files for each speaker\n",
        "    combined_wav = []\n",
        "    male_counter, female_counter = 1, 1\n",
        "\n",
        "    speaker_audio_mapping = {}\n",
        "    for speaker in speaker_array_sorted:\n",
        "        gender = speaker[\"gender\"]\n",
        "        if gender == \"male\":\n",
        "            speaker_audio_mapping[speaker[\"speaker\"]] = f\"male{male_counter}.wav\"\n",
        "            male_counter += 1\n",
        "        elif gender == \"female\":\n",
        "            speaker_audio_mapping[speaker[\"speaker\"]] = f\"female{female_counter}.wav\"\n",
        "            female_counter += 1\n",
        "        else:\n",
        "            speaker_audio_mapping[speaker[\"speaker\"]] = \"narrator.wav\"\n",
        "\n",
        "    for speaker_info in extracted_data:\n",
        "        speaker_name = speaker_info[\"speaker\"]\n",
        "        quotes = speaker_info[\"quote\"]\n",
        "\n",
        "        speaker_wav_path = speaker_audio_mapping.get(speaker_name)\n",
        "\n",
        "        if speaker_wav_path:\n",
        "            for quote_id, text in quotes.items():\n",
        "                cleaned_text = text.replace(\"_\", \"\")\n",
        "                if cleaned_text:\n",
        "                    wav = tts.tts(text=cleaned_text, speaker_wav=speaker_wav_path, language=\"en\")\n",
        "                    combined_wav.append(wav)\n",
        "        else:\n",
        "            print(f\"Warning: No audio file found for speaker '{speaker_name}'\")\n",
        "    samplerate = 22050\n",
        "    combined_wav = np.concatenate(combined_wav)\n",
        "    output_audio_filename = \"combined_audio.wav\"\n",
        "    sf.write(output_audio_filename, combined_wav, samplerate)\n",
        "    print(f\"Audio saved successfully as {output_audio_filename}\")\n",
        "    return jsonify({\n",
        "        \"message\": \"PDF processed, text formatted, speakers identified, genders allocated, and audio generated.\",\n",
        "        \"audio_url\": \"/audio\"\n",
        "    }), 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-oKlXiRz-Xu"
      },
      "outputs": [],
      "source": [
        "@app.route('/audio', methods=['GET'])\n",
        "def get_audio():\n",
        "    return send_file(\"combined_audio.wav\", mimetype=\"audio/wav\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSL4hv3Sz_qb"
      },
      "outputs": [],
      "source": [
        "@app.route('/books', methods=['GET'])\n",
        "def get_books():\n",
        "    books = [\n",
        "        {\"id\": 1, \"title\": \"Streetcar Named Desire\", \"author\": \"Tennessee Williams\", \"cover\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSvjolO0FpddsiWRuBXOZJ7_EKSNA76EUe4BA&s\"},\n",
        "        {\"id\": 2, \"title\": \"The Tragedie of Macbeth\", \"author\": \"William Shakespeare\", \"cover\": \"https://m.media-amazon.com/images/I/81yB3mQGm7L._UF1000,1000_QL80_.jpg\"},\n",
        "        {\"id\": 3, \"title\": \"Antigone\", \"author\": \"Sophocles\", \"cover\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTtBiBb0solzCupMuRPdtcy5fWiEOsLfoSxsA&s\"},\n",
        "    ]\n",
        "    response = Response(json.dumps(books), mimetype=\"application/json\")\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FArCb_28T9Z2"
      },
      "outputs": [],
      "source": [
        "@app.after_request\n",
        "def add_cors_headers(response):\n",
        "    response.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n",
        "    response.headers[\"Access-Control-Allow-Methods\"] = \"GET, POST, OPTIONS\"\n",
        "    response.headers[\"Access-Control-Allow-Headers\"] = \"Content-Type, Authorization\"\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwSEjEN0B8l",
        "outputId": "e4e9ec95-caf2-4d19-81b8-48d668243d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://d51c-34-90-239-171.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Nov/2024 05:26:19] \"GET /books HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Nov/2024 05:26:20] \"GET /books HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset saved to play_speaker_quote_pair.json\n",
            "Dataset saved to speaker_data.json\n",
            " > Text splitted to sentences.\n",
            "['FURILLO stands in the courtroom, visibly tense.', 'THE DIRECTOR watches him with a calm but firm demeanor.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Processing time: 31.08173131942749\n",
            " > Real-time factor: 3.9018501525401725\n",
            " > Text splitted to sentences.\n",
            "['you can’t do this …']\n",
            " > Processing time: 8.170303583145142\n",
            " > Real-time factor: 3.5175568964454538\n",
            " > Text splitted to sentences.\n",
            "['THE DIRECTOR leans forward slightly, unbothered.']\n",
            " > Processing time: 13.313661098480225\n",
            " > Real-time factor: 3.8864412627288836\n",
            " > Text splitted to sentences.\n",
            "['i can and i will.']\n",
            " > Processing time: 9.741756916046143\n",
            " > Real-time factor: 3.8656374171972616\n",
            " > Text splitted to sentences.\n",
            "['FURILLO straightens his posture, his voice steady but determined.']\n",
            " > Processing time: 21.180678606033325\n",
            " > Real-time factor: 3.7766363959036973\n",
            " > Text splitted to sentences.\n",
            "['i am a lawyer in good standing in this court.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Nov/2024 05:32:34] \"POST /upload HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Processing time: 16.72950768470764\n",
            " > Real-time factor: 3.312074813674432\n",
            "Audio saved successfully as combined_audio.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Nov/2024 05:32:59] \"GET /audio HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "ngrok.connect(5000)\n",
        "app.run()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}